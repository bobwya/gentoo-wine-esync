--- null	1970-01-01 01:00:00.000000000 +0100
+++ b/0088-ntdll-Get-rid-of-the-per-event-spinlock-for-auto-res.patch	2020-04-12 23:48:31.992354435 +0100
@@ -0,0 +1,207 @@
+From c41dc5b8c422be3914cd31c239ec586a091b8a3b Mon Sep 17 00:00:00 2001
+From: Zebediah Figura <zfigura@codeweavers.com>
+Date: Mon, 10 Jun 2019 11:25:34 -0400
+Subject: [PATCH] ntdll: Get rid of the per-event spinlock for auto-reset
+ events.
+It's not necessary. Much like semaphores, the shm state is just a hint.
+---
+ dlls/ntdll/esync.c | 74 +++++++++++++++++++++++++++++++++++-----------
+ server/esync.c     | 32 +++++++++++++-------
+ 2 files changed, 78 insertions(+), 28 deletions(-)
+diff --git a/dlls/ntdll/esync.c b/dlls/ntdll/esync.c
+index 2f030c141..87f303403 100644
+--- a/dlls/ntdll/esync.c
++++ b/dlls/ntdll/esync.c
+@@ -570,6 +570,14 @@ static inline void small_pause(void)
+  * problem at all.
+  */
+ 
++/* Removing this spinlock is harder than it looks. esync_wait_objects() can
++ * deal with inconsistent state well enough, and a race between SetEvent() and
++ * ResetEvent() gives us license to yield either result as long as we act
++ * consistently, but that's not enough. Notably, esync_wait_objects() should
++ * probably act like a fence, so that the second half of esync_set_event() does
++ * not seep past a subsequent reset. That's one problem, but no guarantee there
++ * aren't others. */
++
+ NTSTATUS esync_set_event( HANDLE handle, LONG *prev )
+ {
+     static const uint64_t value = 1;
+@@ -583,21 +591,36 @@ NTSTATUS esync_set_event( HANDLE handle, LONG *prev )
+     if ((ret = get_object( handle, &obj ))) return ret;
+     event = obj->shm;
+ 
+-    /* Acquire the spinlock. */
+-    while (interlocked_cmpxchg( &event->locked, 1, 0 ))
+-        small_pause();
++    if (obj->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Acquire the spinlock. */
++        while (interlocked_cmpxchg( &event->locked, 1, 0 ))
++            small_pause();
++    }
++
++    /* For manual-reset events, as long as we're in a lock, we can take the
++     * optimization of only calling write() if the event wasn't already
++     * signaled.
++     *
++     * For auto-reset events, esync_wait_objects() must grab the kernel object.
++     * Thus if we got into a race so that the shm state is signaled but the
++     * eventfd is unsignaled (i.e. reset shm, set shm, set fd, reset fd), we
++     * *must* signal the fd now, or any waiting threads will never wake up. */
+ 
+     /* Only bother signaling the fd if we weren't already signaled. */
+-    if (!(current = interlocked_xchg( &event->signaled, 1 )))
++    if (!(current = interlocked_xchg( &event->signaled, 1 )) || obj->type == ESYNC_AUTO_EVENT)
+     {
+         if (write( obj->fd, &value, sizeof(value) ) == -1)
+-            return FILE_GetNtStatus();
++            ERR("write: %s
+", strerror(errno));
+     }
+ 
+     if (prev) *prev = current;
+ 
+-    /* Release the spinlock. */
+-    event->locked = 0;
++    if (obj->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Release the spinlock. */
++        event->locked = 0;
++    }
+ 
+     return STATUS_SUCCESS;
+ }
+@@ -615,21 +638,34 @@ NTSTATUS esync_reset_event( HANDLE handle, LONG *prev )
+     if ((ret = get_object( handle, &obj ))) return ret;
+     event = obj->shm;
+ 
+-    /* Acquire the spinlock. */
+-    while (interlocked_cmpxchg( &event->locked, 1, 0 ))
+-        small_pause();
++    if (obj->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Acquire the spinlock. */
++        while (interlocked_cmpxchg( &event->locked, 1, 0 ))
++            small_pause();
++    }
+ 
+-    /* Only bother signaling the fd if we weren't already signaled. */
+-    if ((current = interlocked_xchg( &event->signaled, 0 )))
++    /* For manual-reset events, as long as we're in a lock, we can take the
++     * optimization of only calling read() if the event was already signaled.
++     *
++     * For auto-reset events, we have no guarantee that the previous "signaled"
++     * state is actually correct. We need to leave both states unsignaled after
++     * leaving this function, so we always have to read(). */
++    if ((current = interlocked_xchg( &event->signaled, 0 )) || obj->type == ESYNC_AUTO_EVENT)
+     {
+-        /* we don't care about the return value */
+-        read( obj->fd, &value, sizeof(value) );
++        if (read( obj->fd, &value, sizeof(value) ) == -1 && errno != EWOULDBLOCK && errno != EAGAIN)
++        {
++            ERR("read: %s
+", strerror(errno));
++        }
+     }
+ 
+     if (prev) *prev = current;
+ 
+-    /* Release the spinlock. */
+-    event->locked = 0;
++    if (obj->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Release the spinlock. */
++        event->locked = 0;
++    }
+ 
+     return STATUS_SUCCESS;
+ }
+@@ -844,8 +880,9 @@ static void update_grabbed_object( struct esync *obj )
+     else if (obj->type == ESYNC_AUTO_EVENT)
+     {
+         struct event *event = obj->shm;
+-        /* We don't have to worry about a race between this and read(), for
+-         * reasons described near esync_set_event(). */
++        /* We don't have to worry about a race between this and read(), since
++         * this is just a hint, and the real state is in the kernel object.
++         * This might already be 0, but that's okay! */
+         event->signaled = 0;
+     }
+ }
+@@ -1094,6 +1131,7 @@ static NTSTATUS __esync_wait_objects( DWORD count, const HANDLE *handles,
+                         }
+                         else
+                         {
++                            /* FIXME: Could we check the poll or shm state first? Should we? */
+                             if ((size = read( fds[i].fd, &value, sizeof(value) )) == sizeof(value))
+                             {
+                                 /* We found our object. */
+diff --git a/server/esync.c b/server/esync.c
+index 4521993d4..84d0951cb 100644
+--- a/server/esync.c
++++ b/server/esync.c
+@@ -396,9 +396,12 @@ void esync_set_event( struct esync *esync )
+     if (debug_level)
+         fprintf( stderr, "esync_set_event() fd=%d
+", esync->fd );
+ 
+-    /* Acquire the spinlock. */
+-    while (interlocked_cmpxchg( &event->locked, 1, 0 ))
+-        small_pause();
++    if (esync->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Acquire the spinlock. */
++        while (interlocked_cmpxchg( &event->locked, 1, 0 ))
++            small_pause();
++    }
+ 
+     if (!interlocked_xchg( &event->signaled, 1 ))
+     {
+@@ -406,8 +409,11 @@ void esync_set_event( struct esync *esync )
+             perror( "esync: write" );
+     }
+ 
+-    /* Release the spinlock. */
+-    event->locked = 0;
++    if (esync->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Release the spinlock. */
++        event->locked = 0;
++    }
+ }
+ 
+ void esync_reset_event( struct esync *esync )
+@@ -421,9 +427,12 @@ void esync_reset_event( struct esync *esync )
+     if (debug_level)
+         fprintf( stderr, "esync_reset_event() fd=%d
+", esync->fd );
+ 
+-    /* Acquire the spinlock. */
+-    while (interlocked_cmpxchg( &event->locked, 1, 0 ))
+-        small_pause();
++    if (esync->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Acquire the spinlock. */
++        while (interlocked_cmpxchg( &event->locked, 1, 0 ))
++            small_pause();
++    }
+ 
+     /* Only bother signaling the fd if we weren't already signaled. */
+     if (interlocked_xchg( &event->signaled, 0 ))
+@@ -432,8 +441,11 @@ void esync_reset_event( struct esync *esync )
+         read( esync->fd, &value, sizeof(value) );
+     }
+ 
+-    /* Release the spinlock. */
+-    event->locked = 0;
++    if (esync->type == ESYNC_MANUAL_EVENT)
++    {
++        /* Release the spinlock. */
++        event->locked = 0;
++    }
+ }
+ 
+ DECL_HANDLER(create_esync)
+-- 
+2.23.0
